# config.yaml - Configuration file for molecular DDPM training

# General settings
seed: 42
device: 'cuda'  # 'cuda' or 'cpu'
output_dir: 'outputs/ddpm_training'

# Data configuration
data:
  processed_dir: 'data/processed_test'
  train_path: 'data/processed_test/train.pkl'
  test_path: 'data/processed_test/test.pkl'
  val_path: null  # Use test as validation if not specified
  
  batch_size: 8
  shuffle: true
  num_workers: 4
  pin_memory: true

# Model architecture
model:
  # ViSNet dynamics parameters
  hidden_dim: 256
  num_layers: 6
  num_heads: 8
  cutoff: 5.0
  activation: 'silu'
  
  # Edge cutoffs for different interaction types
  edge_cutoff_ligand: 5.0
  edge_cutoff_pocket: 8.0
  edge_cutoff_interaction: 5.0
  lmax: 2
  vecnorm_type: 'max_min'
  trainable_vecnorm: true
  
  # DDPM parameters
  timesteps: 1000
  parametrization: 'eps'
  noise_schedule: 'cosine'  # 'cosine', 'linear', 'polynomial_2', 'learned'
  noise_precision: 1e-4
  loss_type: 'vlb'  # 'vlb' or 'l2'
  
  # Normalization
  norm_values: [1.0, 1.0]
  norm_biases: [null, 0.0]
  
  # Pocket coordinate updates
  update_pocket_coords: false  # Keep pocket fixed during diffusion

# Training configuration
training:
  epochs: 200
  clip_grad: 1.0
  save_every: 10
  
  # Data parameters
  include_pocket: true
  max_atoms: 50
  augment: true

# Optimizer configuration
optimizer:
  type: 'adamw'  # 'adam' or 'adamw'
  lr: 0.0001
  weight_decay: 0.00001
  betas: [0.9, 0.999]

# Learning rate scheduler
scheduler:
  type: 'cosine'  # 'cosine', 'step', or null for no scheduler
  min_lr: 0.000001
  step_size: 50
  gamma: 0.5

# Loss configuration
loss:
  loss_type: 'vlb'
  weighted_loss: true

# Weights & Biases logging
use_wandb: true
wandb:
  project: 'molecular-ddpm-generation'
  entity: null  # Your W&B entity/username
  run_name: null  # Auto-generated if not specified
  tags: ['molecular', 'ddpm', 'drug-discovery', 'crossdock']

# Evaluation (for periodic evaluation during training)
evaluation:
  evaluate_every: 20  # Epochs
  num_samples: 100
  sample_timesteps: 1000
  
# Model checkpointing
checkpointing:
  save_best: true
  save_last: true
  save_every_n_epochs: 10
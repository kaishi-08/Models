# src/models/pocket_encoder.py - UPGRADED to use YOUR advanced EGNN
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool, global_max_pool, knn_graph
from torch_geometric.nn.pool import radius_graph
from torch_geometric.utils import to_dense_batch
import numpy as np

# Import YOUR advanced EGNN
from .egnn import EGNN, EGNNBackbone, ChemicalConstraints

# Safe global pooling function
def safe_global_pool(x, batch, pool_type='mean'):
    try:
        if pool_type == 'mean':
            return global_mean_pool(x, batch)
        else:
            return global_max_pool(x, batch)
    except Exception as e:
        # CPU fallback
        if x.is_cuda:
            x_cpu = x.cpu()
            batch_cpu = batch.cpu()
            try:
                if pool_type == 'mean':
                    result = global_mean_pool(x_cpu, batch_cpu)
                else:
                    result = global_max_pool(x_cpu, batch_cpu)
                return result.cuda()
            except:
                pass
        
        # Manual pooling fallback
        unique_batch = torch.unique(batch)
        pooled = []
        for b in unique_batch:
            mask = batch == b
            if pool_type == 'mean':
                pooled.append(torch.mean(x[mask], dim=0))
            else:
                pooled.append(torch.max(x[mask], dim=0)[0])
        return torch.stack(pooled)


class SmartPocketAtomSelector:
    """Smart pocket atom selection strategies"""
    
    @staticmethod
    def select_by_distance_to_ligand(pocket_pos: torch.Tensor, ligand_pos: torch.Tensor, 
                                   max_atoms: int, primary_radius: float = 6.0, 
                                   secondary_radius: float = 12.0):
        """Distance-based selection with primary/secondary zones"""
        ligand_center = torch.mean(ligand_pos, dim=0)
        distances = torch.norm(pocket_pos - ligand_center, dim=1)
        
        # Priority 1: Atoms within primary radius (direct interaction)
        primary_mask = distances <= primary_radius
        primary_indices = torch.where(primary_mask)[0]
        
        if len(primary_indices) >= max_atoms:
            _, top_indices = torch.topk(distances[primary_indices], 
                                       k=max_atoms, largest=False)
            return primary_indices[top_indices]
        
        # Priority 2: Extend to secondary radius
        secondary_mask = (distances > primary_radius) & (distances <= secondary_radius)
        secondary_indices = torch.where(secondary_mask)[0]
        
        remaining_slots = max_atoms - len(primary_indices)
        if len(secondary_indices) > remaining_slots:
            _, top_secondary = torch.topk(distances[secondary_indices], 
                                         k=remaining_slots, largest=False)
            secondary_indices = secondary_indices[top_secondary]
        
        combined_indices = torch.cat([primary_indices, secondary_indices]) if len(secondary_indices) > 0 else primary_indices
        return combined_indices
    
    @staticmethod
    def select_by_binding_site_prediction(pocket_pos: torch.Tensor, pocket_x: torch.Tensor,
                                        ligand_pos: torch.Tensor, max_atoms: int):
        """Enhanced binding site prediction using chemical features"""
        ligand_center = torch.mean(ligand_pos, dim=0)
        
        # Distance component
        distances = torch.norm(pocket_pos - ligand_center, dim=1)
        distance_scores = torch.exp(-distances / 8.0)  # 8√Ö decay
        
        # Enhanced chemical feature component
        if pocket_x.size(1) >= 7:  # Has full chemical features
            chemical_scores = torch.zeros(pocket_x.size(0), device=pocket_x.device)
            
            # Residue type importance (enhanced)
            res_type = pocket_x[:, 0].long()
            # Important residue types for binding: ARG(1), LYS(11), ASP(3), GLU(6), PHE(13), TRP(17), TYR(18), HIS(8)
            important_residues = torch.tensor([1, 11, 3, 6, 13, 17, 18, 8], device=pocket_x.device)
            for res in important_residues:
                chemical_scores[res_type == res] += 0.3
            
            # Chemical property bonuses
            if pocket_x.size(1) > 2:  # Has hydrophobic
                chemical_scores += pocket_x[:, 2] * 0.25  # Hydrophobic
            if pocket_x.size(1) > 3:  # Has charged  
                chemical_scores += pocket_x[:, 3] * 0.35  # Charged (very important)
            if pocket_x.size(1) > 4:  # Has polar
                chemical_scores += pocket_x[:, 4] * 0.20  # Polar
            if pocket_x.size(1) > 5:  # Has aromatic
                chemical_scores += pocket_x[:, 5] * 0.30  # Aromatic (œÄ-œÄ interactions)
            if pocket_x.size(1) > 6:  # Has B-factor (flexibility)
                # Lower B-factor = more rigid = better for binding
                flexibility_scores = 1.0 / (pocket_x[:, 6] / 50.0 + 1.0)
                chemical_scores += flexibility_scores * 0.15
        else:
            chemical_scores = torch.ones(pocket_x.size(0), device=pocket_x.device)
        
        # Combined score with enhanced weighting
        combined_scores = distance_scores * 0.6 + chemical_scores * 0.4
        
        # Select top atoms
        _, selected_indices = torch.topk(combined_scores, k=min(max_atoms, len(combined_scores)), largest=True)
        return selected_indices
    
    @staticmethod
    def select_by_surface_accessibility(pocket_pos: torch.Tensor, ligand_pos: torch.Tensor,
                                      max_atoms: int, probe_radius: float = 1.4):
        """Select surface-accessible atoms (simplified version)"""
        ligand_center = torch.mean(ligand_pos, dim=0)
        distances = torch.norm(pocket_pos - ligand_center, dim=1)
        
        # Simple surface approximation: atoms not too crowded
        pairwise_distances = torch.cdist(pocket_pos, pocket_pos)
        neighbor_counts = (pairwise_distances < probe_radius * 2).sum(dim=1) - 1  # Exclude self
        
        # Prefer atoms with fewer neighbors (more surface-like) but close to ligand
        surface_scores = 1.0 / (neighbor_counts.float() + 1.0)
        distance_scores = torch.exp(-distances / 10.0)
        
        combined_scores = surface_scores * 0.4 + distance_scores * 0.6
        
        _, selected_indices = torch.topk(combined_scores, k=min(max_atoms, len(combined_scores)), largest=True)
        return selected_indices
    
    @staticmethod
    def select_by_residue_importance(pocket_pos: torch.Tensor, pocket_x: torch.Tensor,
                                   ligand_pos: torch.Tensor, max_atoms: int):
        """Select based on residue type importance for drug binding"""
        ligand_center = torch.mean(ligand_pos, dim=0)
        distances = torch.norm(pocket_pos - ligand_center, dim=1)
        
        # Residue importance weights
        residue_weights = torch.ones(pocket_x.size(0), device=pocket_x.device)
        
        if pocket_x.size(1) > 0:
            res_type = pocket_x[:, 0].long()
            
            # High importance: charged, aromatic, special residues
            high_importance = torch.tensor([1, 3, 6, 8, 11, 13, 17, 18], device=pocket_x.device)  # ARG, ASP, GLU, HIS, LYS, PHE, TRP, TYR
            for res in high_importance:
                residue_weights[res_type == res] = 3.0
            
            # Medium importance: polar, hydrophobic
            medium_importance = torch.tensor([2, 5, 9, 10, 12, 15, 16, 19], device=pocket_x.device)  # ASN, GLN, ILE, LEU, MET, SER, THR, VAL
            for res in medium_importance:
                residue_weights[res_type == res] = 2.0
        
        # Distance decay
        distance_scores = torch.exp(-distances / 10.0)
        
        # Combined score
        combined_scores = residue_weights * distance_scores
        
        _, selected_indices = torch.topk(combined_scores, k=min(max_atoms, len(combined_scores)), largest=True)
        return selected_indices
    
    @staticmethod
    def select_adaptive(pocket_pos: torch.Tensor, pocket_x: torch.Tensor,
                       ligand_pos: torch.Tensor, max_atoms: int, strategy: str = "adaptive"):
        """Adaptive selection combining multiple strategies"""
        
        if strategy == "distance":
            return SmartPocketAtomSelector.select_by_distance_to_ligand(
                pocket_pos, ligand_pos, max_atoms
            )
        elif strategy == "binding_site":
            return SmartPocketAtomSelector.select_by_binding_site_prediction(
                pocket_pos, pocket_x, ligand_pos, max_atoms
            )
        elif strategy == "surface":
            return SmartPocketAtomSelector.select_by_surface_accessibility(
                pocket_pos, ligand_pos, max_atoms
            )
        elif strategy == "residue":
            return SmartPocketAtomSelector.select_by_residue_importance(
                pocket_pos, pocket_x, ligand_pos, max_atoms
            )
        else:  # adaptive - use enhanced binding site prediction
            return SmartPocketAtomSelector.select_by_binding_site_prediction(
                pocket_pos, pocket_x, ligand_pos, max_atoms
            )


class ImprovedProteinPocketEncoder(nn.Module):
    """
    üéØ IMPROVED: Pocket encoder using YOUR advanced EGNN from egnn.py
    
    Key improvements over simple version:
    - Uses your full EGNN with chemical constraints
    - Smart atom selection strategies  
    - Multi-scale pocket representation
    - Chemical-aware encoding
    - SE(3) equivariance from your EGNN
    - Advanced attention mechanisms
    """
    
    def __init__(self, node_features: int = 8, edge_features: int = 4,
                 hidden_dim: int = 128, num_layers: int = 3, 
                 output_dim: int = 256, max_radius: float = 10.0,
                 max_pocket_atoms: int = 1000, 
                 selection_strategy: str = "adaptive",
                 use_chemical_constraints: bool = True,
                 enable_se3_equivariance: bool = True):
        super().__init__()
        
        self.node_features = node_features
        self.edge_features = edge_features
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.max_pocket_atoms = max_pocket_atoms
        self.selection_strategy = selection_strategy
        self.max_radius = max_radius
        self.use_chemical_constraints = use_chemical_constraints
        
        # Smart atom selector
        self.atom_selector = SmartPocketAtomSelector()
        
        # Flexible embeddings for different input dimensions
        self.node_embedding_6d = nn.Linear(6, hidden_dim)
        self.node_embedding_7d = nn.Linear(7, hidden_dim)
        self.node_embedding_8d = nn.Linear(8, hidden_dim)
        self.node_embedding_9d = nn.Linear(9, hidden_dim)
        
        # üéØ YOUR ADVANCED EGNN (with all chemical constraints & SE(3) equivariance)
        self.pocket_egnn = EGNN(
            in_node_nf=hidden_dim,
            in_edge_nf=edge_features,
            hidden_nf=hidden_dim,
            n_layers=num_layers,
            attention=True,
            norm_diff=True,
            tanh=True,
            coords_range=10.0,
            sin_embedding=True,  # Advanced distance embedding
            normalization_factor=100,
            aggregation_method='sum',
            reflection_equiv=enable_se3_equivariance,
            enable_chemical_constraints=use_chemical_constraints,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        # Multi-scale feature extraction (enhanced)
        self.local_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        self.global_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Position-aware processing (enhanced)
        self.position_mlp = nn.Sequential(
            nn.Linear(3, hidden_dim // 4),
            nn.SiLU(),
            nn.Linear(hidden_dim // 4, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, hidden_dim)
        )
        
        # Enhanced feature fusion
        self.feature_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim * 2),  # More capacity
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim * 2),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # Advanced attention mechanism
        self.attention_weights = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # Output projection with residual connection
        self.output_projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        print(f"‚úÖ Improved PocketEncoder with YOUR EGNN created")
        print(f"   Strategy: {selection_strategy}, Max atoms: {max_pocket_atoms}")
        print(f"   Chemical constraints: {use_chemical_constraints}")
        print(f"   SE(3) equivariance: {enable_se3_equivariance}")
        print(f"   EGNN layers: {num_layers}, Hidden dim: {hidden_dim}")
    
    def forward(self, x: torch.Tensor, pos: torch.Tensor, edge_index: torch.Tensor = None,
                batch: torch.Tensor = None, ligand_pos: torch.Tensor = None, 
                atom_types: torch.Tensor = None, **kwargs):
        
        # Smart atom selection (if needed)
        if x.size(0) > self.max_pocket_atoms and ligand_pos is not None:
            try:
                selected_indices = self.atom_selector.select_adaptive(
                    pocket_pos=pos, 
                    pocket_x=x, 
                    ligand_pos=ligand_pos,
                    max_atoms=self.max_pocket_atoms,
                    strategy=self.selection_strategy
                )
                
                x = x[selected_indices]
                pos = pos[selected_indices]
                if batch is not None:
                    batch = batch[selected_indices]
                if atom_types is not None:
                    atom_types = atom_types[selected_indices]
                    
                # Update edge_index with selected atoms
                if edge_index is not None:
                    edge_mask = torch.isin(edge_index[0], selected_indices) & torch.isin(edge_index[1], selected_indices)
                    if edge_mask.any():
                        edge_index = edge_index[:, edge_mask]
                        # Remap indices
                        old_to_new = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(selected_indices)}
                        edge_index = torch.tensor([[old_to_new[edge_index[0, i].item()], old_to_new[edge_index[1, i].item()]] 
                                                 for i in range(edge_index.size(1)) 
                                                 if edge_index[0, i].item() in old_to_new and edge_index[1, i].item() in old_to_new]).t()
                    else:
                        edge_index = torch.zeros((2, 0), dtype=torch.long, device=x.device)
                
            except Exception as e:
                print(f"Smart selection failed: {e}, using random fallback")
                indices = torch.randperm(x.size(0))[:self.max_pocket_atoms]
                x = x[indices]
                pos = pos[indices]
                if batch is not None:
                    batch = batch[indices]
                if atom_types is not None:
                    atom_types = atom_types[indices]
        
        # Flexible feature embedding
        h = self._embed_features_flexible(x)
        
        # Enhanced position encoding
        pos_features = self.position_mlp(pos)
        
        # Feature fusion with residual connection
        h_with_pos = self.feature_fusion(torch.cat([h, pos_features, h], dim=-1))
        h_normalized = self.layer_norm(h_with_pos + h)  # Residual connection
        
        # Build edge index if not provided (using radius_graph)
        if edge_index is None or edge_index.size(1) == 0:
            if h.size(0) > 1:
                edge_index = radius_graph(pos, r=self.max_radius, 
                                        max_num_neighbors=min(32, h.size(0)-1))
            else:
                edge_index = torch.zeros((2, 0), dtype=torch.long, device=h.device)
        
        # üéØ YOUR ADVANCED EGNN PROCESSING (with all chemical constraints)
        try:
            # Convert atom_types for chemical constraints if available
            if atom_types is None and x.size(1) > 0:
                # Try to extract atom types from residue features
                atom_types = self._extract_pseudo_atom_types(x)
            
            # Process through your advanced EGNN
            h_processed, pos_updated = self.pocket_egnn(
                h=h_normalized, 
                x=pos,  # Positions for SE(3) processing
                edge_index=edge_index,
                atom_types=atom_types  # For chemical constraints
            )
            
            # Extract constraint losses if available
            constraint_loss = getattr(self.pocket_egnn, 'total_constraint_loss', torch.tensor(0.0, device=h.device))
            
        except Exception as e:
            print(f"EGNN processing error: {e}, using fallback")
            h_processed = h_normalized
            pos_updated = pos
            constraint_loss = torch.tensor(0.0, device=h.device)
        
        # Multi-scale processing with your EGNN output
        h_local = self.local_processor(h_processed)
        h_global = self.global_processor(h_processed)
        h_combined = h_local + h_global + h_processed  # Multi-scale fusion
        
        # Advanced attention-based pooling
        if batch is not None:
            try:
                # Multi-head attention pooling
                h_attended, _ = self.attention_weights(
                    h_combined.unsqueeze(0), h_combined.unsqueeze(0), h_combined.unsqueeze(0)
                )
                h_attended = h_attended.squeeze(0)
                
                pocket_mean = safe_global_pool(h_attended, batch, 'mean')
                pocket_max = safe_global_pool(h_combined, batch, 'max')
                pocket_repr = pocket_mean + pocket_max * 0.3  # Weighted combination
                
            except Exception as e:
                print(f"Attention pooling error: {e}, using simple pooling")
                pocket_repr = safe_global_pool(h_combined, batch, 'mean')
        else:
            # Single pocket case with attention
            try:
                h_attended, _ = self.attention_weights(
                    h_combined.unsqueeze(0), h_combined.unsqueeze(0), h_combined.unsqueeze(0)
                )
                pocket_repr = torch.mean(h_attended.squeeze(0), dim=0, keepdim=True)
            except:
                pocket_repr = torch.mean(h_combined, dim=0, keepdim=True)
        
        # Final output projection
        output = self.output_projection(pocket_repr)
        
        # Store constraint loss for potential use in training
        if hasattr(self, 'constraint_loss'):
            self.constraint_loss = constraint_loss
        
        return output
    
    def _embed_features_flexible(self, x: torch.Tensor) -> torch.Tensor:
        """Flexible feature embedding for different input dimensions"""
        if x.dim() != 2:
            raise ValueError(f"Expected 2D input, got {x.dim()}D: {x.shape}")
        
        input_dim = x.size(1)
        
        if input_dim == 6:
            return self.node_embedding_6d(x.float())
        elif input_dim == 7:
            return self.node_embedding_7d(x.float())
        elif input_dim == 8:
            return self.node_embedding_8d(x.float())
        elif input_dim == 9:
            return self.node_embedding_9d(x.float())
        elif input_dim < 6:
            # Pad to 6D
            padding = torch.zeros(x.size(0), 6 - input_dim, device=x.device, dtype=x.dtype)
            x_padded = torch.cat([x, padding], dim=1)
            return self.node_embedding_6d(x_padded.float())
        elif input_dim > 9:
            # Truncate to 9D
            x_truncated = x[:, :9]
            return self.node_embedding_9d(x_truncated.float())
        else:
            # Pad to 8D
            padding = torch.zeros(x.size(0), 8 - input_dim, device=x.device, dtype=x.dtype)
            x_padded = torch.cat([x, padding], dim=1)
            return self.node_embedding_8d(x_padded.float())
    
    def _extract_pseudo_atom_types(self, x: torch.Tensor) -> torch.Tensor:
        """Extract pseudo atom types from residue features for chemical constraints"""
        # Map residue types to representative atom types for constraint checking
        # This is a simplified mapping - in practice you might want more sophisticated mapping
        
        if x.size(1) > 0:
            residue_types = x[:, 0].long()  # First feature is usually residue type
            
            # Map residue types to representative atom types for constraints
            # Using carbon (6) as default, nitrogen (7) for charged, oxygen (8) for polar
            atom_types = torch.full_like(residue_types, 6)  # Default: carbon
            
            # Charged residues ‚Üí nitrogen representation
            charged_residues = torch.tensor([1, 11, 3, 6, 8], device=x.device)  # ARG, LYS, ASP, GLU, HIS
            for res in charged_residues:
                atom_types[residue_types == res] = 7
                
            # Polar residues ‚Üí oxygen representation
            polar_residues = torch.tensor([15, 16, 18], device=x.device)  # SER, THR, TYR
            for res in polar_residues:
                atom_types[residue_types == res] = 8
            
            return atom_types
        else:
            # Fallback: all carbon
            return torch.full((x.size(0),), 6, device=x.device, dtype=torch.long)


# Factory function for creating improved pocket encoder using YOUR EGNN
def create_improved_pocket_encoder(hidden_dim: int = 256, output_dim: int = 256, 
                                 selection_strategy: str = "adaptive",
                                 use_chemical_constraints: bool = True,
                                 num_layers: int = 3, **kwargs):
    """Create improved pocket encoder using YOUR EGNN with all features"""
    
    return ImprovedProteinPocketEncoder(
        node_features=8,
        hidden_dim=hidden_dim,
        output_dim=output_dim,
        num_layers=num_layers,
        selection_strategy=selection_strategy,
        use_chemical_constraints=use_chemical_constraints,
        enable_se3_equivariance=True,
        **kwargs
    )


# üéØ Simple pocket encoder for basic usage (backward compatibility)
class SimplePocketEncoder(nn.Module):
    """Simple pocket encoder fallback"""
    
    def __init__(self, input_dim: int = 7, hidden_dim: int = 256, output_dim: int = 256):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x: torch.Tensor, pos: torch.Tensor = None, **kwargs):
        # Simple mean pooling over pocket atoms
        pocket_features = torch.mean(x, dim=0, keepdim=True)
        return self.encoder(pocket_features)


# Test function
def test_improved_pocket_encoder():
    """Test the improved pocket encoder with YOUR EGNN"""
    print("Testing Improved Pocket Encoder with YOUR EGNN...")
    
    # Create test data
    num_residues = 500
    num_ligand_atoms = 25
    
    pocket_x = torch.randn(num_residues, 7)
    pocket_pos = torch.randn(num_residues, 3) * 15
    pocket_batch = torch.zeros(num_residues, dtype=torch.long)
    
    ligand_pos = torch.randn(num_ligand_atoms, 3) * 3
    
    print(f"\nüß™ Test data: {num_residues} residues, {num_ligand_atoms} ligand atoms")
    
    # Test different strategies
    strategies = ["adaptive", "distance", "binding_site", "surface", "residue"]
    
    for strategy in strategies:
        print(f"\n   Testing strategy: {strategy}")
        
        try:
            # Create encoder
            encoder = create_improved_pocket_encoder(
                hidden_dim=128,
                output_dim=256,
                selection_strategy=strategy,
                use_chemical_constraints=True,
                num_layers=3
            )
            
            # Test forward pass
            pocket_repr = encoder(
                x=pocket_x,
                pos=pocket_pos,
                batch=pocket_batch,
                ligand_pos=ligand_pos
            )
            
            print(f"     Input: {num_residues} residues ‚Üí Output: {pocket_repr.shape}")
            print(f"     ‚úÖ Strategy '{strategy}' successful")
            
            # Check if constraint loss is available
            if hasattr(encoder, 'constraint_loss'):
                print(f"     Constraint loss: {encoder.constraint_loss.item():.4f}")
            
        except Exception as e:
            print(f"     ‚ùå Strategy '{strategy}' failed: {e}")
    
    print(f"\n‚úÖ Improved pocket encoder with YOUR EGNN working!")


if __name__ == "__main__":
    test_improved_pocket_encoder()